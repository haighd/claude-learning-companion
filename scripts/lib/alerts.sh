#!/bin/bash
# Alert system for Emergent Learning Framework
#
# Provides alert generation and management for critical events
#
# Usage:
#   source "$(dirname "${BASH_SOURCE[0]}")/../lib/alerts.sh"
#   alerts_init
#   alert_trigger "critical" "Database error rate > 10%" error_rate="15.5"
#   alert_check_disk_space
#   alert_check_error_rate

ALERTS_DIR="${ALERTS_DIR:-}"
ALERTS_CEO_DIR="${ALERTS_CEO_DIR:-}"
METRICS_DB="${METRICS_DB:-}"

# Alert severities
declare -A ALERT_SEVERITIES=(
    [info]=0
    [warning]=1
    [critical]=2
    [emergency]=3
)

#
# Initialize alert system
#
# Args:
#   $1 - Base directory (optional, defaults to ~/.claude/clc)
#
alerts_init() {
    local base_dir="${1:-}"

    if [ -z "$base_dir" ]; then
        local home_dir="${HOME:-$USERPROFILE}"
        if [ -n "$home_dir" ]; then
            base_dir="$home_dir/.claude/clc"
        else
            base_dir="/tmp/clc"
        fi
    fi

    ALERTS_DIR="$base_dir/alerts"
    ALERTS_CEO_DIR="$base_dir/ceo-inbox"
    METRICS_DB="$base_dir/memory/index.db"

    # Create alerts directory
    mkdir -p "$ALERTS_DIR" 2>/dev/null || true
    mkdir -p "$ALERTS_CEO_DIR" 2>/dev/null || true

    # Create active alerts tracking file
    touch "$ALERTS_DIR/.active_alerts" 2>/dev/null || true

    return 0
}

#
# Trigger an alert
#
# Args:
#   $1 - Severity (info|warning|critical|emergency)
#   $2 - Message
#   ${@:3} - Additional context as key=value pairs
#
alert_trigger() {
    local severity="$1"
    local message="$2"
    shift 2

    if [ -z "$ALERTS_DIR" ]; then
        echo "ERROR: Alerts not initialized. Call alerts_init first." >&2
        return 1
    fi

    local severity_level="${ALERT_SEVERITIES[$severity]:-0}"
    local timestamp=$(date -u '+%Y-%m-%dT%H:%M:%SZ')
    local alert_id="alert_$(date +%s)_$$"

    # Build context
    local context=""
    for arg in "$@"; do
        context+="$arg "
    done

    # Create alert file
    local alert_file="$ALERTS_DIR/${alert_id}.alert"
    cat > "$alert_file" <<EOF
ALERT: $severity
TIME: $timestamp
MESSAGE: $message
CONTEXT: $context
STATUS: active
EOF

    # Log the alert (if logging is available)
    if declare -f log_error > /dev/null 2>&1; then
        case "$severity" in
            critical|emergency)
                log_error "ALERT: $message" severity="$severity" $@
                ;;
            warning)
                log_warn "ALERT: $message" severity="$severity" $@
                ;;
            *)
                log_info "ALERT: $message" severity="$severity" $@
                ;;
        esac
    fi

    # Record metric
    if declare -f metrics_record > /dev/null 2>&1; then
        metrics_record "alert_count" 1 severity="$severity" $@
    fi

    # For critical/emergency, escalate to CEO inbox
    if [ "$severity_level" -ge 2 ]; then
        _alert_escalate_to_ceo "$severity" "$message" "$context"
    fi

    # Add to active alerts
    echo "$alert_id|$severity|$timestamp|$message" >> "$ALERTS_DIR/.active_alerts"

    return 0
}

#
# Escalate alert to CEO inbox
#
_alert_escalate_to_ceo() {
    local severity="$1"
    local message="$2"
    local context="$3"

    local ceo_file="$ALERTS_CEO_DIR/alert_$(date +%Y%m%d_%H%M%S).md"

    cat > "$ceo_file" <<EOF
# ALERT: $severity

**Time**: $(date '+%Y-%m-%d %H:%M:%S')
**Severity**: $severity
**Status**: Needs Review

## Message

$message

## Context

$context

## Recommended Actions

$(case "$message" in
    *"error rate"*)
        echo "1. Review recent error logs"
        echo "2. Check for pattern in failures"
        echo "3. Consider rollback if recent deploy"
        ;;
    *"disk space"*)
        echo "1. Review log rotation settings"
        echo "2. Clean up old backups if safe"
        echo "3. Consider storage expansion"
        ;;
    *"backup"*)
        echo "1. Investigate backup failure logs"
        echo "2. Verify backup destination accessible"
        echo "3. Test restore capability"
        ;;
    *)
        echo "1. Investigate root cause"
        echo "2. Review related logs and metrics"
        echo "3. Determine remediation plan"
        ;;
esac)

## Resolution

_To be filled by human reviewer_

---
Generated by: Emergent Learning Framework Alert System
EOF

    echo "CEO alert created: $ceo_file"
}

#
# Check disk space and alert if low
#
# Args:
#   $1 - Threshold in MB (default 100)
#
alert_check_disk_space() {
    local threshold_mb="${1:-100}"

    if [ -z "$ALERTS_DIR" ]; then
        return 1
    fi

    # Get available space for emergent learning directory
    local home_dir="${HOME:-$USERPROFILE}"
    local target_dir="$home_dir/.claude/clc"

    # Platform-specific disk space check
    local avail_mb=0
    if df --version 2>&1 | grep -q GNU; then
        # GNU df
        avail_mb=$(df -BM "$target_dir" 2>/dev/null | awk 'NR==2 {gsub(/M/,"",$4); print $4}')
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS
        avail_mb=$(df -m "$target_dir" 2>/dev/null | awk 'NR==2 {print $4}')
    else
        # Fallback: try to parse df output
        avail_mb=$(df "$target_dir" 2>/dev/null | awk 'NR==2 {print int($4/1024)}')
    fi

    # Default to 0 if we couldn't determine
    avail_mb=${avail_mb:-0}

    if [ "$avail_mb" -lt "$threshold_mb" ] && [ "$avail_mb" -gt 0 ]; then
        alert_trigger "critical" "Disk space critically low: ${avail_mb}MB available" \
            threshold="${threshold_mb}MB" \
            location="$target_dir"
        return 1
    fi

    return 0
}

#
# Check error rate and alert if high
#
# Args:
#   $1 - Threshold percentage (default 10)
#   $2 - Time window in hours (default 1)
#
alert_check_error_rate() {
    local threshold_percent="${1:-10}"
    local hours="${2:-1}"

    if [ -z "$METRICS_DB" ] || [ ! -f "$METRICS_DB" ]; then
        return 0  # No metrics, can't check
    fi

    # Query error rate from metrics
    local error_rate=$(sqlite3 "$METRICS_DB" <<SQL 2>/dev/null || echo "0"
SELECT
    ROUND(
        CAST(SUM(CASE WHEN tags LIKE '%status:failure%' THEN metric_value ELSE 0 END) AS REAL) /
        CAST(SUM(metric_value) AS REAL) * 100,
        2
    )
FROM metrics
WHERE metric_name = 'operation_count'
  AND timestamp > datetime('now', '-$hours hours');
SQL
)

    error_rate=${error_rate:-0}

    # Compare with threshold (handle decimal comparison)
    if awk -v rate="$error_rate" -v thresh="$threshold_percent" 'BEGIN {exit !(rate > thresh)}'; then
        alert_trigger "critical" "Error rate exceeded threshold: ${error_rate}%" \
            threshold="${threshold_percent}%" \
            time_window="${hours}h"
        return 1
    fi

    return 0
}

#
# Check backup status and alert if failed
#
alert_check_backup_status() {
    local backup_log="${ALERTS_DIR}/../logs/backup.log"

    if [ ! -f "$backup_log" ]; then
        return 0  # No backup log, assume OK
    fi

    # Check if last backup failed (look for ERROR in last 50 lines)
    if tail -50 "$backup_log" 2>/dev/null | grep -q "ERROR.*backup"; then
        local last_error=$(tail -50 "$backup_log" | grep "ERROR" | tail -1)
        alert_trigger "critical" "Backup system failure detected" \
            last_error="$last_error"
        return 1
    fi

    return 0
}

#
# Run all health checks
#
alert_health_check() {
    local checks_passed=0
    local checks_failed=0

    echo "Running health checks..."

    if alert_check_disk_space; then
        echo "  [OK] Disk space"
        ((checks_passed++))
    else
        echo "  [FAIL] Disk space"
        ((checks_failed++))
    fi

    if alert_check_error_rate; then
        echo "  [OK] Error rate"
        ((checks_passed++))
    else
        echo "  [FAIL] Error rate"
        ((checks_failed++))
    fi

    if alert_check_backup_status; then
        echo "  [OK] Backup status"
        ((checks_passed++))
    else
        echo "  [FAIL] Backup status"
        ((checks_failed++))
    fi

    echo ""
    echo "Health check complete: $checks_passed passed, $checks_failed failed"

    return $checks_failed
}

#
# Clear an alert (mark as resolved)
#
# Args:
#   $1 - Alert ID
#
alert_clear() {
    local alert_id="$1"

    if [ -z "$ALERTS_DIR" ]; then
        return 1
    fi

    local alert_file="$ALERTS_DIR/${alert_id}.alert"

    if [ -f "$alert_file" ]; then
        sed -i.bak 's/STATUS: active/STATUS: resolved/' "$alert_file"
        echo "Alert cleared: $alert_id"
        return 0
    else
        echo "Alert not found: $alert_id"
        return 1
    fi
}

#
# List active alerts
#
alert_list_active() {
    if [ -z "$ALERTS_DIR" ]; then
        return 1
    fi

    echo "Active Alerts:"
    echo "=============="

    if [ ! -f "$ALERTS_DIR/.active_alerts" ]; then
        echo "(none)"
        return 0
    fi

    local count=0
    while IFS='|' read -r alert_id severity timestamp message; do
        local alert_file="$ALERTS_DIR/${alert_id}.alert"
        if [ -f "$alert_file" ] && grep -q "STATUS: active" "$alert_file"; then
            echo "[$severity] $timestamp - $message"
            echo "  ID: $alert_id"
            ((count++))
        fi
    done < "$ALERTS_DIR/.active_alerts"

    if [ $count -eq 0 ]; then
        echo "(none)"
    else
        echo ""
        echo "Total: $count active alerts"
    fi

    return 0
}

# Export functions
export -f alert_trigger alert_clear alert_list_active
export -f alert_check_disk_space alert_check_error_rate alert_check_backup_status
export -f alert_health_check
